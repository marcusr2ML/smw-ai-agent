This is my first uploaded project to github. It is a work in progress. I want to add a nueral network of somekind (probably VAE) to provide a latent representation of my play style, and also a custom policy learned from my gameplay. These parts are in the works


The agent is implemented with a double-Q learning algorithm using the Bell-man equation. A convolutional nueral network is trained on the game state and the resultant reward based off the policy where motion is encoded in the training by stacking 4 consecutive frames of gameplay in each training step. An epsilon-greedy approach is used to explore the action space where the bot performs a random action if a rand_int<epsilon is choosen; otherwise, the optimal action determined by the cnn is taken. Taking the optimal action is a greedy apporach, so to insetivize reward a replay buffer of the above cnn is saved every num_save epochs of training and is used in the above Bell-man equation to determine the loss of our current nework. Notice that this means the ccn after training picks an optimal step as a greedy algorithm would, but this choice is crafted around future reward. 

The output space is simply the number of button combinations the bot is allowed to hit (taken to be a subset of a human players) and the frames are pixelated to a lower resolution as well as greyscaled to reduce the number of color channels in the ccn. To train the ai agent the main program needs to be ran. It forms an instance of smw_gym_retro and the agent of the Agent class using the double-Q learning algorithm. There are several modules present before this exploratory loop. 

First data is collected by capturing real time gameplay using a class HumanDataCollect to train a variational autoencoder (VAE) to learn the schematics of the expert users play style. The class saves a bunch of segments set by checkpoints for a set of playthroughs and combines them to form a statistical ensemble. The main purpose is to train mario to move foward and run. A convolutional nueral network will be used to acquire the inputs that are feed into the VAE. Note a double Q-learning algorithm can also be trained by using these segments, just drop the epsilon greedy approach till the exploration stage. 
